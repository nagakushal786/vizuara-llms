{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d85c9cc",
   "metadata": {},
   "source": [
    "## Data Sampling using Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de1086",
   "metadata": {},
   "source": [
    "### Creating Input-Target pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fefd489",
   "metadata": {},
   "source": [
    "We will implement a `data loader` that fetches input-target pairs using a `sliding window` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19834cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"verdict.txt\", 'r', encoding='utf-8-sig') as f:\n",
    "  raw_text=f.read()\n",
    "\n",
    "enc_text=tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90250258",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample=enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448a28a",
   "metadata": {},
   "source": [
    "- Now the most intuitive way to create input-target pairs for the nextword prediction is to consider 2 variables x, y.\n",
    "\n",
    "- $x$ contains the input tokens and $y$ contains the targets, which are the inputs shifted by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a92d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size=4\n",
    "\n",
    "x=enc_sample[:context_size]\n",
    "y=enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4239d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "  context=enc_sample[:i]\n",
    "  desired=enc_sample[i]\n",
    "\n",
    "  print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "  context=enc_sample[:i]\n",
    "  desired=enc_sample[i]\n",
    "\n",
    "  context_decoded=tokenizer.decode(context)\n",
    "  desired_decoded=tokenizer.decode([desired])\n",
    "\n",
    "  print(context_decoded, \"---->\", desired_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72306f5",
   "metadata": {},
   "source": [
    "Now we have to implement an efficient data loader that iterates over the input dataset and returns inputs and targets as `PyTorch tensors`, which can be thought of as multi-dimensional arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a367c5",
   "metadata": {},
   "source": [
    "### Implementing Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9fade",
   "metadata": {},
   "source": [
    "- We will be using PyTorch's built-in `Dataset` and `DataLoader` classes.\n",
    "\n",
    "- Step 1: Tokenize the entire text.\n",
    "\n",
    "- Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length.\n",
    "\n",
    "- Step 3: Return the total number of rows in the dataset.\n",
    "\n",
    "- Step 4: Returning a single row from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd12404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import tensor\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids=[]\n",
    "    self.target_ids=[]\n",
    "\n",
    "    # Tokenize the entire text\n",
    "    token_ids=tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "    # Apply the sliding window approach to chunk the dataset\n",
    "    for i in range(0, len(token_ids)-max_length, stride):\n",
    "      input_chunk=token_ids[i:i+max_length]\n",
    "      target_chunk=token_ids[i+1:i+max_length+1]\n",
    "      self.input_ids.append(tensor(input_chunk))\n",
    "      self.target_ids.append(tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fe301",
   "metadata": {},
   "source": [
    "- Step 1: Initialize the tokenizer.\n",
    "\n",
    "- Step 2: Create dataset.\n",
    "\n",
    "- Step 3: `drop_last=True` drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
    "\n",
    "- Step 4: Number of CPU processes to use for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader_v1(txt, batch_size=4, max_length=256,\n",
    "                          stride=128, shuffle=True, drop_last=True,\n",
    "                          num_workers=0):\n",
    "  # Initialize the tokenizer\n",
    "  tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "  # Create dataset\n",
    "  dataset=GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "  # Create dataloader\n",
    "  dataloader=DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=shuffle, drop_last=drop_last,\n",
    "                        num_workers=num_workers)\n",
    "  \n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56515c2",
   "metadata": {},
   "source": [
    "- `batch_size`: The number of batches the model processes at once before updating it's parameters.\n",
    "\n",
    "- `num_workers`: For parallel processing on different threads of CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=create_data_loader_v1(\n",
    "  raw_text,\n",
    "  batch_size=1,\n",
    "  max_length=4,\n",
    "  stride=1,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "data_iter=iter(dataloader)\n",
    "first_batch=next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch=next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e93926",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=create_data_loader_v1(\n",
    "  raw_text,\n",
    "  batch_size=8,\n",
    "  max_length=4,\n",
    "  stride=4,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "data_iter=iter(dataloader)\n",
    "inputs, targets=next(data_iter)\n",
    "\n",
    "print(f\"Inputs: {inputs}\")\n",
    "print(f\"Targets: {targets}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
