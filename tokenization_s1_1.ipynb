{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f98d54",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7c5d7",
   "metadata": {},
   "source": [
    "### Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d2af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"verdict.txt\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "  raw_text=f.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(f\"The first 100 characters: {raw_text[:99]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Splitting based on white-spaces\n",
    "text=\"Hello, world. This, is a text.\"\n",
    "result=re.split(r'(\\s)', text)\n",
    "print(result)\n",
    "\n",
    "# Splitting based on white-spaces, commas and full-stops\n",
    "result=re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf68e9",
   "metadata": {},
   "source": [
    "Here even the `white-spaces` are being considered as tokens. So first we have to remove them from the result. For now to reduce memory consumption, we will remove white-spaces. Later we will look at the tokenization scheme including white-spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.strip() will be false for white-spaces\n",
    "result=[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hello, world. Is this-- a test?\"\n",
    "\n",
    "# Splitting based on all kind of punctuations\n",
    "result=re.split(r'([,.;:?_!\"()\\']|--|\\s)', text)\n",
    "result=[item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying on our verdict.txt\n",
    "pre_processed_text=re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "pre_processed_text=[item.strip() for item in pre_processed_text if item.strip()]\n",
    "\n",
    "print(pre_processed_text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pre_processed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73599e86",
   "metadata": {},
   "source": [
    "### Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19105e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=sorted(set(pre_processed_text))\n",
    "vocab_size=len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b339db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary={token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(vocabulary.items()):\n",
    "  print(item)\n",
    "\n",
    "  if i>=20:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6e4d4",
   "metadata": {},
   "source": [
    "### Tokenizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int=vocab\n",
    "    self.int_to_str={i:s for s, i in vocab.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    pre_processed=re.split(r'([,.;:?_!\"()\\']|--|\\s)', text)\n",
    "    pre_processed=[item.strip() for item in pre_processed if item.strip()]\n",
    "    ids=[self.str_to_int[s] for s in pre_processed]\n",
    "    return ids\n",
    "  \n",
    "  def decode(self, ids):\n",
    "    text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "    # Replacing spaces before the specified punctuations\n",
    "    text=re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40831824",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=SimpleTokenizerV1(vocabulary)\n",
    "\n",
    "# This text is present in the dataset\n",
    "text=\"\"\"\"It's the last he painted, you know,\"\n",
    "         Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485196e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tokenizer.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a76d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text which is not present in the dataset\n",
    "text=\"Hello, I am Kushal\"\n",
    "\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e323b",
   "metadata": {},
   "source": [
    "<pre>\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[26], line 4\n",
    "      1 # Text which is not present in the dataset\n",
    "      2 text = \"Hello, I am Kushal\"\n",
    "----> 4 ids = tokenizer.encode(text)\n",
    "      5 print(ids)\n",
    "\n",
    "Cell In[23], line 9, in SimpleTokenizerV1.encode(self, text)\n",
    "      7 pre_processed = re.split(r'([,.;:?_!\"()\\']|--|\\s)', text)\n",
    "      8 pre_processed = [item.strip() for item in pre_processed if item.strip()]\n",
    "----> 9 ids = [self.str_to_int[s] for s in pre_processed]\n",
    "     10 return ids\n",
    "\n",
    "KeyError: 'Hello'\n",
    "</pre>\n",
    "\n",
    "That's why we have to use large and diverse datasets in order to avoid such errors. To avoid this, we use `Special context tokens`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e80c7ba",
   "metadata": {},
   "source": [
    "### Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd32d6",
   "metadata": {},
   "source": [
    "`<|unk|>`: To handle with the tokens that are not present in the dataset.  \n",
    "`<|endoftext|>`: To seperate multiple text sources for better computation and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc469b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens=sorted(list(set(pre_processed_text)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab={token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5eb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec53351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int=vocab\n",
    "    self.int_to_str={i:s for s, i in vocab.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    pre_processed=re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    pre_processed=[item.strip() for item in pre_processed if item.strip()]\n",
    "    pre_processed=[\n",
    "      item if item in self.str_to_int\n",
    "      else \"<|unk|>\" for item in pre_processed\n",
    "    ]\n",
    "    ids=[self.str_to_int[s] for s in pre_processed]\n",
    "    return ids\n",
    "  \n",
    "  def decode(self, ids):\n",
    "    text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "    # Replacing spaces before the specified punctuations\n",
    "    text=re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1=\"Hello, do you like tea?\"\n",
    "text2=\"In the sunlight terraces of the palace.\"\n",
    "\n",
    "text=\" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a69ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text=tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96930c7",
   "metadata": {},
   "source": [
    "`[BOS] (beginning of sequence)`: This token marks the start of a text. It signifies the LLM where a piece of content begins.  \n",
    "`[EOS] (end of sequence)`: This token is positioned at end of text and is used to concatenate multiple unrelated texts similar to <|endoftext|>.  \n",
    "`[PAD] (padding)`: When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are padded using the [PAD] token, up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae305a1",
   "metadata": {},
   "source": [
    "- The tokenizer used for GPT models does not need any of these tokens mentioned but only uses `<|endoftext|>` for simplicity.\n",
    "\n",
    "- To deal with out-of-vocabulary words, instead of <|unk|>, GPT uses `byte pair encoding tokenizer`, which breaks down words into subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1ba08",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd775bd7",
   "metadata": {},
   "source": [
    "`tiktoken` is a fast BPE tokenizer for use with OpenAI's models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def969e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074efcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=(\n",
    "  \"Hello, do you like tea? <|endoftext|> In the sunlight terraces\"\n",
    "  \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers=tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48cf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings=tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abedc0b8",
   "metadata": {},
   "source": [
    "- The <|endoftext|> token is assigned to a relatively large token ID 50256.\n",
    "\n",
    "- The BPE tokenizer has a vocabulary size of 50257 with <|endoftext|> being assigned the largest token ID.\n",
    "\n",
    "- The BPE tokenizer encodes and decodes unknown words such as `someunknownPlace` correctly.\n",
    "\n",
    "- The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters and solves `out-of-vocabulary` problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "integers=tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings=tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
